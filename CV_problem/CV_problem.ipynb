{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "N6ORIsL2zT2g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HR85-xlPtT5-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#**Building a CNN model for classification of the given dataset**\n"
      ]
    },
    {
      "metadata": {
        "id": "Qu20XkmJ5hw9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dest = Path('Data')  #creating a folder to load data into \n",
        "dest.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5QK-oQsRvhUg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading the data from file into variables and checking out the labels."
      ]
    },
    {
      "metadata": {
        "id": "CZGK2wO1vdCb",
        "colab_type": "code",
        "outputId": "3323603e-0700-4579-f5a6-cdd364848bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "with open(dest/'train_image.pkl', 'rb') as f:\n",
        "    data_img = np.array(pickle.load(f),dtype='float32').reshape(8000,28,28)  # reshaping the each data into 2D matrix\n",
        "with open(dest/'train_label.pkl', 'rb') as f:\n",
        "    data_label = np.array(pickle.load(f))\n",
        "set(data_label.flatten())  #looking at all the set of labeles"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 2, 3, 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "OHFtUxF5t1yh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now the data set has 4 labels (0,2,3,6). The following function maps these valuse to default labels 0,1,2 and 3 for the convenience of model building. We then use the function on the extracted labels."
      ]
    },
    {
      "metadata": {
        "id": "yNCcF7bHHbcE",
        "colab_type": "code",
        "outputId": "eafe22ec-1921-477f-8aed-912c76e70206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def map_vect(x):\n",
        "  if x==0:\n",
        "    return 0\n",
        "  elif x==2:\n",
        "    return 1\n",
        "  elif x==3:\n",
        "    return 2\n",
        "  else:\n",
        "    return 3\n",
        "\n",
        "data_label_mapped = np.array([map_vect(x) for x in data_label])\n",
        "set(data_label_mapped.flatten())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "N6aYzQLw3nk-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setting up for splitting data into train and validation set"
      ]
    },
    {
      "metadata": {
        "id": "ujsEozFl9v8R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_size = len(data_img)    #total number of images\n",
        "validation_split = .2  #ratio of data to be put into validation set\n",
        "indices = list(range(data_size))  #getting the indices in form of list\n",
        "split = int(np.floor(validation_split * data_size))  #amount of data to be split into validation set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pjtPBvAHZpE8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random_seed= 77\n",
        "np.random.seed(random_seed)\n",
        "np.random.shuffle(indices)  #shuffling the indices randomly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ry5BJgIm5Dq9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Splitting the data into train_set and valid_set"
      ]
    },
    {
      "metadata": {
        "id": "KovKmu5UIL7d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "train_img = data_img[train_indices[:]]    #train set images\n",
        "train_label = data_label_mapped[train_indices[:]]    #train set labels\n",
        "valid_img = data_img[val_indices[:]]    #validarion set images\n",
        "valid_label = data_label_mapped[val_indices[:]]    #validation set images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J5I5l8kt5rk0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualising the data set using matplotlib to display images"
      ]
    },
    {
      "metadata": {
        "id": "4y9obj4RIkFc",
        "colab_type": "code",
        "outputId": "27e9f6d7-dbe1-41c2-aec4-0858ed7f21bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(1, 9):\n",
        "    axis = plt.subplot(2,4,i)\n",
        "    img = train_img[i]\n",
        "    plt.imshow(img,cmap=\"gray\")\n",
        "    axis.set_title(str(int(train_label[i])))\n",
        "    axis.axis('off')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADuCAYAAAAgAly4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "5BliqFfS6L3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating Tensor data set and data loader of train_set and validation set. \n",
        "Pytorchâ€™s DataLoader is responsible for managing batches. You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches and also automatically creates mini batches for you thus refactoring a lot of coding."
      ]
    },
    {
      "metadata": {
        "id": "Ozrn52_wJssF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bs = 16   #batch size\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = map(\n",
        "    torch.tensor, (train_img, train_label, valid_img, valid_label)\n",
        ")\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs)\n",
        "\n",
        "valid_ds = TensorDataset(x_valid, y_valid)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs*2)  #we are keeping batch size for validation set more as it does not require back propogation and hence less memory is needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TG38rnqVq4CX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below is the implementation of function used for calculating the accuracy of our model on validation set after each epoc"
      ]
    },
    {
      "metadata": {
        "id": "BYPZJHLqbP0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(y_hat,y):\n",
        "  _,topclass = y_hat.topk(1,dim=1) \n",
        "  topclass.view(bs*2)    #getting the predicted classes for each input image\n",
        "  eq = topclass==y.view(topclass.shape)  #comparing the prediction with the actual class label\n",
        "  acc = torch.mean(eq.type(torch.FloatTensor))   # calculating the accuracy\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XzsvbI59HTwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function below calucaltes loss of training and validation mini batch and also runs back propogation in case of training batch. If parameter opt (Optimiser) = None we know that it is validation batch and hence dont run back prop. However if opt !=None we run back prop taking the case for training batch."
      ]
    },
    {
      "metadata": {
        "id": "vtK1EU8E9Xbp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_batch(model, loss_func, x_batch, y_batch, opt=None):\n",
        "    y_hat = model(x_batch)  #prediction of medel for the given batch\n",
        "\n",
        "    loss = loss_func(y_hat, y_batch)\n",
        "    acc = None\n",
        "\n",
        "    if opt != None:     # if optimiser is passed as a parameter then we take it as the case of training batch and run back propagation\n",
        "        \n",
        "        #backpro\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        \n",
        "    else:               # if no optimiser is provided , we instead calculate the accuracy for the validation batch\n",
        "      \n",
        "        acc = accuracy(y_hat,y_batch) \n",
        "     \n",
        "\n",
        "    return loss.item(), len(x_batch), acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGfozR65JAjC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "fit function implemented below trains the model for given number of epochs passed to it as parameter. It also prints the result as training loss ,validation loss and accuracy after each epoc"
      ]
    },
    {
      "metadata": {
        "id": "qA8sAtY-CNEG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "  \n",
        "    model.cuda()   #instruction to train the model on gpu\n",
        " \n",
        "    for epoch in range(epochs):   \n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        \n",
        "        for x_batch, y_batch in train_dl: #looping over mini batches\n",
        "            x_batch = x_batch.cuda()\n",
        "            y_batch = y_batch.cuda()\n",
        "            l, n, a = loss_batch(model, loss_func, x_batch, y_batch, opt)\n",
        "            train_loss = train_loss+l\n",
        "            \n",
        "        train_loss = train_loss/len(train_dl) #calculating average loss\n",
        "\n",
        "        model.eval()\n",
        "        acc = None\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            losses, nums, acc = zip(\n",
        "                *[loss_batch(model, loss_func, x_batch.cuda(), y_batch.cuda()) for x_batch, y_batch in valid_dl]\n",
        "            )\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "        acc = np.sum(np.multiply(acc, nums)) / np.sum(nums)\n",
        "\n",
        "        print(epoch, train_loss, val_loss, acc ,\"(Train loss, Validation loss, Validation Accuracy)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pVfl0COtP7P5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## First model Implementation\n",
        "\n",
        "We will quickly build a basic model and see how it is performing on the data set. We cannot have a deep/big network like vgg nets as amount of data available is not sufficient to effeciently train sucha a large model. So we will build a relatively shallow network.\n",
        "\n",
        "X ---> Conv1 ---> Relu ---> Conv2 ---> Relu ---> BatchNorm1 ---> MaxPool ---> Conv3 --->Relu ---> Conv4 ---> Relu ---> BatchNorm2 ---> MaxPool ---> Conv5 --->Relu ---> Conv6 ---> Relu ---> BatchNorm3 ---> FC1 --->Relu ---> FC2 ---> Relu ---> FC3 ---> LogSoftmax --->Y"
      ]
    },
    {
      "metadata": {
        "id": "h5kQiGy0P8g7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Mnist_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.maxpool = nn.MaxPool2d(2,2)\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1,padding = 1)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1,padding = 1)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding = 1)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=1,padding = 1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding = 1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1,padding = 1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        \n",
        "        \n",
        "        self.fc1 = nn.Linear(7*7*256,1000)\n",
        "        self.fc2 = nn.Linear(1000,100)\n",
        "        self.fc3 = nn.Linear(100,4)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        xb = xb.view(-1, 1, 28, 28)\n",
        "\n",
        "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
        "\n",
        "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "\n",
        "        xb = self.maxpool(xb)\n",
        "\n",
        "        xb = F.relu(self.bn3(self.conv3(xb)))\n",
        "\n",
        "        xb = F.relu(self.bn4(self.conv4(xb)))\n",
        "\n",
        "        xb = self.maxpool(xb)\n",
        "\n",
        "        xb = F.relu(self.bn5(self.conv5(xb)))\n",
        "        \n",
        "        xb = F.relu(self.bn6(self.conv6(xb)))\n",
        "        \n",
        "\n",
        "        xb = xb.view(-1,7*7*256)\n",
        "        \n",
        "        xb = F.relu(self.fc1(xb))\n",
        "\n",
        "        xb = F.relu(self.fc2(xb))\n",
        "        \n",
        "        xb = self.fc3(xb)\n",
        "        \n",
        "        xb = F.log_softmax(xb,dim=1)\n",
        "\n",
        "        return xb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GDfl1DzjQLnS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Trainig the given model for 30 epochs. We can see the model starts overfitting just after 6-7 epochs as training validation loss starts increasing .\n",
        "So we can stop training the model early to prevnt over fitting"
      ]
    },
    {
      "metadata": {
        "id": "wKC7IYDZRVuz",
        "colab_type": "code",
        "outputId": "cc250edc-0c15-43bf-f18f-2e9c792aacbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "cell_type": "code",
      "source": [
        "loss_func = nn.NLLLoss()\n",
        "model = Mnist_CNN()\n",
        "opt = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "fit(30, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.5536320560239255 0.5883753442764282 0.821875 (Train loss, Validation loss, Validation Accuracy)\n",
            "1 0.3829866089858115 0.4802435202896595 0.8425 (Train loss, Validation loss, Validation Accuracy)\n",
            "2 0.30976767707616093 0.5620652040839196 0.838125 (Train loss, Validation loss, Validation Accuracy)\n",
            "3 0.2630756007414311 0.4779361373186111 0.843125 (Train loss, Validation loss, Validation Accuracy)\n",
            "4 0.23051006568595767 0.4576839543879032 0.85625 (Train loss, Validation loss, Validation Accuracy)\n",
            "5 0.18722562754526734 0.5149331326782703 0.868125 (Train loss, Validation loss, Validation Accuracy)\n",
            "6 0.1665618116874248 0.4708438678085804 0.856875 (Train loss, Validation loss, Validation Accuracy)\n",
            "7 0.14401511906646192 0.5445600724220276 0.856875 (Train loss, Validation loss, Validation Accuracy)\n",
            "8 0.11934659225866198 0.6872199022769928 0.84375 (Train loss, Validation loss, Validation Accuracy)\n",
            "9 0.09893122905865312 0.6604534660279751 0.8575 (Train loss, Validation loss, Validation Accuracy)\n",
            "10 0.0921797876805067 0.6872244918346405 0.84625 (Train loss, Validation loss, Validation Accuracy)\n",
            "11 0.07680146904662251 0.5833913791179657 0.859375 (Train loss, Validation loss, Validation Accuracy)\n",
            "12 0.07229105422273278 0.61589375436306 0.840625 (Train loss, Validation loss, Validation Accuracy)\n",
            "13 0.05742673967964947 0.7920405936241149 0.845625 (Train loss, Validation loss, Validation Accuracy)\n",
            "14 0.061188781457021835 0.695185829102993 0.863125 (Train loss, Validation loss, Validation Accuracy)\n",
            "15 0.05354828662006184 0.7535491636395455 0.8575 (Train loss, Validation loss, Validation Accuracy)\n",
            "16 0.03637661469168961 0.8490332552790641 0.855625 (Train loss, Validation loss, Validation Accuracy)\n",
            "17 0.04707250622101128 0.7885652147233486 0.853125 (Train loss, Validation loss, Validation Accuracy)\n",
            "18 0.04391516594216228 0.7666027753055096 0.85375 (Train loss, Validation loss, Validation Accuracy)\n",
            "19 0.029760013446211816 0.8084990194439888 0.874375 (Train loss, Validation loss, Validation Accuracy)\n",
            "20 0.036915352595970036 0.7662430139631033 0.865 (Train loss, Validation loss, Validation Accuracy)\n",
            "21 0.027961217695847154 0.7240584155917168 0.855625 (Train loss, Validation loss, Validation Accuracy)\n",
            "22 0.030133627420291305 0.7987674391269683 0.86375 (Train loss, Validation loss, Validation Accuracy)\n",
            "23 0.025926779862493275 0.8370603328943252 0.860625 (Train loss, Validation loss, Validation Accuracy)\n",
            "24 0.023284577857702972 0.7539515794813633 0.855625 (Train loss, Validation loss, Validation Accuracy)\n",
            "25 0.02503566232509911 0.8330962491780519 0.861875 (Train loss, Validation loss, Validation Accuracy)\n",
            "26 0.023359175184741617 1.0197545087337494 0.84875 (Train loss, Validation loss, Validation Accuracy)\n",
            "27 0.03606943401508033 0.7464435216784477 0.855 (Train loss, Validation loss, Validation Accuracy)\n",
            "28 0.026528523378074167 0.86822226382792 0.8675 (Train loss, Validation loss, Validation Accuracy)\n",
            "29 0.02847579330904409 0.8233938284218312 0.860625 (Train loss, Validation loss, Validation Accuracy)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JcHaq_5wZEUG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Mnist_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.maxpool = nn.MaxPool2d(2,2)\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1,padding = 1)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1,padding = 1)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding = 1)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=1,padding = 1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding = 1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1,padding = 1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        \n",
        "        \n",
        "        self.fc1 = nn.Linear(7*7*256,1000)\n",
        "#         self.fc2 = nn.Linear(1000,100)\n",
        "        self.fc3 = nn.Linear(1000,4)\n",
        "\n",
        "    def forward(self, xb):        \n",
        "        \n",
        "        xb = xb.view(-1, 1, 28, 28)\n",
        "\n",
        "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
        "        \n",
        "        xb = F.dropout(xb,p=0.55)\n",
        "\n",
        "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
        "        \n",
        "        xb = F.dropout(xb,p=0.45)\n",
        "\n",
        "        xb = self.maxpool(xb)\n",
        "\n",
        "        xb = F.relu(self.bn3(self.conv3(xb)))\n",
        "        \n",
        "        xb = F.dropout(xb,p=0.35)\n",
        "\n",
        "        xb = F.relu(self.bn4(self.conv4(xb)))\n",
        "        \n",
        "        xb = F.dropout(xb,p=0.25)\n",
        "\n",
        "        xb = self.maxpool(xb)\n",
        "\n",
        "        xb = F.relu(self.bn5(self.conv5(xb)))\n",
        "        \n",
        "        xb = F.dropout(xb,p=0.15)\n",
        "        \n",
        "        xb = F.relu(self.bn6(self.conv6(xb)))\n",
        "        \n",
        "\n",
        "        xb = xb.view(-1,7*7*256)\n",
        "        \n",
        "        xb = F.relu(self.fc1(xb))\n",
        "        \n",
        "        xb = F.dropout(xb,p=0.25)\n",
        "\n",
        "#         xb = F.relu(self.fc2(xb))\n",
        "        \n",
        "#         xb = F.dropout(xb,p=0.15)\n",
        "        \n",
        "        xb = self.fc3(xb)\n",
        "        \n",
        "        xb = F.log_softmax(xb,dim=1)\n",
        "\n",
        "        \n",
        "\n",
        "        return xb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzW_x0SIZYYe",
        "colab_type": "code",
        "outputId": "9c45a6ad-a4c7-4d64-e2fb-adc4a1f49d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        }
      },
      "cell_type": "code",
      "source": [
        "model = Mnist_CNN()\n",
        "opt = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "fit(50, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7597417671233415 0.6631641918420792 0.723125 (Train loss, Validation loss, Validation Accuracy)\n",
            "1 0.5920050769671797 0.5476444190740586 0.78375 (Train loss, Validation loss, Validation Accuracy)\n",
            "2 0.5097141651064158 0.5325930655002594 0.7925 (Train loss, Validation loss, Validation Accuracy)\n",
            "3 0.4695860850252211 0.4512806940078735 0.819375 (Train loss, Validation loss, Validation Accuracy)\n",
            "4 0.43489276422187684 0.4685099825263023 0.81875 (Train loss, Validation loss, Validation Accuracy)\n",
            "5 0.394864712562412 0.4492850139737129 0.823125 (Train loss, Validation loss, Validation Accuracy)\n",
            "6 0.3836793652176857 0.4207743117213249 0.834375 (Train loss, Validation loss, Validation Accuracy)\n",
            "7 0.36491455329582095 0.4382036176323891 0.8275 (Train loss, Validation loss, Validation Accuracy)\n",
            "8 0.3456278882548213 0.4275685891509056 0.83875 (Train loss, Validation loss, Validation Accuracy)\n",
            "9 0.3264730620197952 0.43679234564304353 0.830625 (Train loss, Validation loss, Validation Accuracy)\n",
            "10 0.3095701682381332 0.42263912796974185 0.830625 (Train loss, Validation loss, Validation Accuracy)\n",
            "11 0.2853327754512429 0.43600561887025835 0.8425 (Train loss, Validation loss, Validation Accuracy)\n",
            "12 0.2772659716568887 0.46101763427257536 0.845 (Train loss, Validation loss, Validation Accuracy)\n",
            "13 0.26239142717793584 0.46353043496608737 0.839375 (Train loss, Validation loss, Validation Accuracy)\n",
            "14 0.25743020774796604 0.4264674288034439 0.849375 (Train loss, Validation loss, Validation Accuracy)\n",
            "15 0.23190647223964334 0.5139873740077019 0.833125 (Train loss, Validation loss, Validation Accuracy)\n",
            "16 0.2234003910049796 0.4762489381432533 0.853125 (Train loss, Validation loss, Validation Accuracy)\n",
            "17 0.22180806130170821 0.5239061377942562 0.848125 (Train loss, Validation loss, Validation Accuracy)\n",
            "18 0.19925064804032444 0.564507070928812 0.828125 (Train loss, Validation loss, Validation Accuracy)\n",
            "19 0.1842630610615015 0.5941872022300959 0.84625 (Train loss, Validation loss, Validation Accuracy)\n",
            "20 0.18841102430596948 0.5274159222841263 0.843125 (Train loss, Validation loss, Validation Accuracy)\n",
            "21 0.1730610933806747 0.5419379252195359 0.840625 (Train loss, Validation loss, Validation Accuracy)\n",
            "22 0.16886360964737832 0.5827547481656075 0.83375 (Train loss, Validation loss, Validation Accuracy)\n",
            "23 0.1583768542855978 0.6370020321011544 0.835625 (Train loss, Validation loss, Validation Accuracy)\n",
            "24 0.1494081209320575 0.5689191493391991 0.84375 (Train loss, Validation loss, Validation Accuracy)\n",
            "25 0.1401956535410136 0.6508742992579937 0.8425 (Train loss, Validation loss, Validation Accuracy)\n",
            "26 0.13052028014790268 0.6463540442287922 0.85 (Train loss, Validation loss, Validation Accuracy)\n",
            "27 0.12728732837364076 0.6111912488937378 0.83875 (Train loss, Validation loss, Validation Accuracy)\n",
            "28 0.12233302657492459 0.6717445376515389 0.853125 (Train loss, Validation loss, Validation Accuracy)\n",
            "29 0.11189687602221966 0.6385968659818172 0.845 (Train loss, Validation loss, Validation Accuracy)\n",
            "30 0.10734968268312514 0.6663686366379261 0.85125 (Train loss, Validation loss, Validation Accuracy)\n",
            "31 0.10590784098021686 0.7036713427305221 0.843125 (Train loss, Validation loss, Validation Accuracy)\n",
            "32 0.10726847009733319 0.722708547860384 0.839375 (Train loss, Validation loss, Validation Accuracy)\n",
            "33 0.09865536618977785 0.7981952612102031 0.84125 (Train loss, Validation loss, Validation Accuracy)\n",
            "34 0.09757422654889523 0.7298621036112308 0.845625 (Train loss, Validation loss, Validation Accuracy)\n",
            "35 0.0792528897151351 0.7082133156061172 0.8425 (Train loss, Validation loss, Validation Accuracy)\n",
            "36 0.08874683822505176 0.7297515001893043 0.825 (Train loss, Validation loss, Validation Accuracy)\n",
            "37 0.08066863850690424 0.7322676777839661 0.841875 (Train loss, Validation loss, Validation Accuracy)\n",
            "38 0.08595518675632775 0.6886788146942854 0.849375 (Train loss, Validation loss, Validation Accuracy)\n",
            "39 0.08080845938064157 0.8012293419241905 0.85625 (Train loss, Validation loss, Validation Accuracy)\n",
            "40 0.06807293294928968 0.7895113943517208 0.855 (Train loss, Validation loss, Validation Accuracy)\n",
            "41 0.07949866449460387 0.7797095160931349 0.843125 (Train loss, Validation loss, Validation Accuracy)\n",
            "42 0.07250897668302059 0.7567237487435341 0.8575 (Train loss, Validation loss, Validation Accuracy)\n",
            "43 0.06973032020032406 0.7875893591344356 0.83625 (Train loss, Validation loss, Validation Accuracy)\n",
            "44 0.06791911118663847 0.8799499872326851 0.851875 (Train loss, Validation loss, Validation Accuracy)\n",
            "45 0.07054535233415664 0.7359857146441936 0.846875 (Train loss, Validation loss, Validation Accuracy)\n",
            "46 0.06610881052911281 0.7700918887555599 0.853125 (Train loss, Validation loss, Validation Accuracy)\n",
            "47 0.07016818601638079 0.7231353142857552 0.846875 (Train loss, Validation loss, Validation Accuracy)\n",
            "48 0.06284530715085566 0.8255562636256218 0.843125 (Train loss, Validation loss, Validation Accuracy)\n",
            "49 0.06192952123703435 1.0784396475553513 0.844375 (Train loss, Validation loss, Validation Accuracy)\n",
            "0 0.7597417671233415 0.6631641918420792 0.723125 (Train loss, Validation loss, Validation Accuracy)\n",
            "1 0.5920050769671797 0.5476444190740586 0.78375 (Train loss, Validation loss, Validation Accuracy)\n",
            "2 0.5097141651064158 0.5325930655002594 0.7925 (Train loss, Validation loss, Validation Accuracy)\n",
            "3 0.4695860850252211 0.4512806940078735 0.819375 (Train loss, Validation loss, Validation Accuracy)\n",
            "4 0.43489276422187684 0.4685099825263023 0.81875 (Train loss, Validation loss, Validation Accuracy)\n",
            "5 0.394864712562412 0.4492850139737129 0.823125 (Train loss, Validation loss, Validation Accuracy)\n",
            "6 0.3836793652176857 0.4207743117213249 0.834375 (Train loss, Validation loss, Validation Accuracy)\n",
            "7 0.36491455329582095 0.4382036176323891 0.8275 (Train loss, Validation loss, Validation Accuracy)\n",
            "8 0.3456278882548213 0.4275685891509056 0.83875 (Train loss, Validation loss, Validation Accuracy)\n",
            "9 0.3264730620197952 0.43679234564304353 0.830625 (Train loss, Validation loss, Validation Accuracy)\n",
            "10 0.3095701682381332 0.42263912796974185 0.830625 (Train loss, Validation loss, Validation Accuracy)\n",
            "11 0.2853327754512429 0.43600561887025835 0.8425 (Train loss, Validation loss, Validation Accuracy)\n",
            "12 0.2772659716568887 0.46101763427257536 0.845 (Train loss, Validation loss, Validation Accuracy)\n",
            "13 0.26239142717793584 0.46353043496608737 0.839375 (Train loss, Validation loss, Validation Accuracy)\n",
            "14 0.25743020774796604 0.4264674288034439 0.849375 (Train loss, Validation loss, Validation Accuracy)\n",
            "15 0.23190647223964334 0.5139873740077019 0.833125 (Train loss, Validation loss, Validation Accuracy)\n",
            "16 0.2234003910049796 0.4762489381432533 0.853125 (Train loss, Validation loss, Validation Accuracy)\n",
            "17 0.22180806130170821 0.5239061377942562 0.848125 (Train loss, Validation loss, Validation Accuracy)\n",
            "18 0.19925064804032444 0.564507070928812 0.828125 (Train loss, Validation loss, Validation Accuracy)\n",
            "19 0.1842630610615015 0.5941872022300959 0.84625 (Train loss, Validation loss, Validation Accuracy)\n",
            "20 0.18841102430596948 0.5274159222841263 0.843125 (Train loss, Validation loss, Validation Accuracy)\n",
            "21 0.1730610933806747 0.5419379252195359 0.840625 (Train loss, Validation loss, Validation Accuracy)\n",
            "22 0.16886360964737832 0.5827547481656075 0.83375 (Train loss, Validation loss, Validation Accuracy)\n",
            "23 0.1583768542855978 0.6370020321011544 0.835625 (Train loss, Validation loss, Validation Accuracy)\n",
            "24 0.1494081209320575 0.5689191493391991 0.84375 (Train loss, Validation loss, Validation Accuracy)\n",
            "25 0.1401956535410136 0.6508742992579937 0.8425 (Train loss, Validation loss, Validation Accuracy)\n",
            "26 0.13052028014790268 0.6463540442287922 0.85 (Train loss, Validation loss, Validation Accuracy)\n",
            "27 0.12728732837364076 0.6111912488937378 0.83875 (Train loss, Validation loss, Validation Accuracy)\n",
            "28 0.12233302657492459 0.6717445376515389 0.853125 (Train loss, Validation loss, Validation Accuracy)\n",
            "29 0.11189687602221966 0.6385968659818172 0.845 (Train loss, Validation loss, Validation Accuracy)\n",
            "30 0.10734968268312514 0.6663686366379261 0.85125 (Train loss, Validation loss, Validation Accuracy)\n",
            "31 0.10590784098021686 0.7036713427305221 0.843125 (Train loss, Validation loss, Validation Accuracy)\n",
            "32 0.10726847009733319 0.722708547860384 0.839375 (Train loss, Validation loss, Validation Accuracy)\n",
            "33 0.09865536618977785 0.7981952612102031 0.84125 (Train loss, Validation loss, Validation Accuracy)\n",
            "34 0.09757422654889523 0.7298621036112308 0.845625 (Train loss, Validation loss, Validation Accuracy)\n",
            "35 0.0792528897151351 0.7082133156061172 0.8425 (Train loss, Validation loss, Validation Accuracy)\n",
            "36 0.08874683822505176 0.7297515001893043 0.825 (Train loss, Validation loss, Validation Accuracy)\n",
            "37 0.08066863850690424 0.7322676777839661 0.841875 (Train loss, Validation loss, Validation Accuracy)\n",
            "38 0.08595518675632775 0.6886788146942854 0.849375 (Train loss, Validation loss, Validation Accuracy)\n",
            "39 0.08080845938064157 0.8012293419241905 0.85625 (Train loss, Validation loss, Validation Accuracy)\n",
            "40 0.06807293294928968 0.7895113943517208 0.855 (Train loss, Validation loss, Validation Accuracy)\n",
            "41 0.07949866449460387 0.7797095160931349 0.843125 (Train loss, Validation loss, Validation Accuracy)\n",
            "42 0.07250897668302059 0.7567237487435341 0.8575 (Train loss, Validation loss, Validation Accuracy)\n",
            "43 0.06973032020032406 0.7875893591344356 0.83625 (Train loss, Validation loss, Validation Accuracy)\n",
            "44 0.06791911118663847 0.8799499872326851 0.851875 (Train loss, Validation loss, Validation Accuracy)\n",
            "45 0.07054535233415664 0.7359857146441936 0.846875 (Train loss, Validation loss, Validation Accuracy)\n",
            "46 0.06610881052911281 0.7700918887555599 0.853125 (Train loss, Validation loss, Validation Accuracy)\n",
            "47 0.07016818601638079 0.7231353142857552 0.846875 (Train loss, Validation loss, Validation Accuracy)\n",
            "48 0.06284530715085566 0.8255562636256218 0.843125 (Train loss, Validation loss, Validation Accuracy)\n",
            "49 0.06192952123703435 1.0784396475553513 0.844375 (Train loss, Validation loss, Validation Accuracy)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}